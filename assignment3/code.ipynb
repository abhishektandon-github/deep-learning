{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc04a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD, Adagrad, RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "verbose = 1\n",
    "epochs = 9999\n",
    "\n",
    "\n",
    "path = '~/Documents/abhishek/assignment/Group_6/'\n",
    "\n",
    "# import logging\n",
    "\n",
    "# logging.getLogger('tensorflow').setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "dict = {0: [1, 0, 0, 0, 0], 3: [0, 1, 0, 0, 0], 4: [0, 0, 1, 0, 0], 8: [0, 0, 0, 1, 0], 9: [0, 0, 0, 0, 1]}\n",
    "\n",
    "set = ['train', 'val', 'test']\n",
    "\n",
    "X = []\n",
    "\n",
    "for t in range(0, 3):\n",
    "     x = []\n",
    "     for i in dict:\n",
    "         new_path = path + f'{set[t]}/{i}/'\n",
    "         for k in os.listdir(new_path):\n",
    "             x.append(cv2.imread(new_path + k, cv2.IMREAD_GRAYSCALE).flatten())\n",
    "     X.append(x)\n",
    "\n",
    "X_train = np.array(X[0])\n",
    "X_val = np.array(X[1])\n",
    "X_test = np.array(X[2])\n",
    "\n",
    "l1 = X_train.shape[0]//5\n",
    "l2 = X_val.shape[0]//5\n",
    "l3 = X_test.shape[0]//5\n",
    "\n",
    "def ground_truth(l):\n",
    "    \n",
    "    gt = []\n",
    "    for i in dict:\n",
    "        gt += l*[dict[i]]\n",
    "\n",
    "    return gt\n",
    "\n",
    "y_train = np.array(ground_truth(l1))\n",
    "y_val = np.array(ground_truth(l2))\n",
    "y_test = np.array(ground_truth(l3))\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "def architecture(hidden):\n",
    "\n",
    "    tf.random.set_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    if len(hidden) == 3:\n",
    "      model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.InputLayer(input_shape=(784,), name='Input_Layer'),\n",
    "      tf.keras.layers.Dense(hidden[0], activation='sigmoid', name='HL1'),\n",
    "      tf.keras.layers.Dense(hidden[1], activation='sigmoid', name='HL2'),\n",
    "      tf.keras.layers.Dense(hidden[2], activation='sigmoid', name='HL3'),\n",
    "      tf.keras.layers.Dense(5, activation='sigmoid', name='Output_Layer')\n",
    "      ])\n",
    "\n",
    "    if len(hidden) == 4:\n",
    "      model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.InputLayer(input_shape=(784,), name='Input_Layer'),\n",
    "      tf.keras.layers.Dense(hidden[0], activation='sigmoid', name='HL1'),\n",
    "      tf.keras.layers.Dense(hidden[1], activation='sigmoid', name='HL2'),\n",
    "      tf.keras.layers.Dense(hidden[2], activation='sigmoid', name='HL3'),\n",
    "      tf.keras.layers.Dense(hidden[3], activation='sigmoid', name='HL4'),\n",
    "      tf.keras.layers.Dense(5, activation='sigmoid', name='Output_Layer')\n",
    "      ])\n",
    "\n",
    "    if len(hidden) == 5:\n",
    "      model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.InputLayer(input_shape=(784,), name='Input_Layer'),\n",
    "      tf.keras.layers.Dense(hidden[0], activation='sigmoid', name='HL1'),\n",
    "      tf.keras.layers.Dense(hidden[1], activation='sigmoid', name='HL2'),\n",
    "      tf.keras.layers.Dense(hidden[2], activation='sigmoid', name='HL3'),\n",
    "      tf.keras.layers.Dense(hidden[3], activation='sigmoid', name='HL4'),\n",
    "      tf.keras.layers.Dense(hidden[4], activation='sigmoid', name='HL5'),\n",
    "      tf.keras.layers.Dense(5, activation='sigmoid', name='Output_Layer')\n",
    "      ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def save_w(hidden):\n",
    "    model = architecture(hidden)\n",
    "    model.save_weights('./weights/'+f'model_{str(hidden)}.h5')\n",
    "\n",
    "\n",
    "def conv_label(y_true, y_pred):\n",
    "\n",
    "  y_pred = to_categorical(np.argmax(y_pred, axis=1), 5)\n",
    "\n",
    "  label = tf.argmax(y_true, axis = 1)\n",
    "  pred = tf.argmax(y_pred, axis = 1)\n",
    "\n",
    "  return label, pred\n",
    "\n",
    "def cm_plot(hidden, conf, string):\n",
    "    \n",
    "    print('\\n',string)\n",
    "    print(conf)\n",
    "    #import seaborn as sb\n",
    "\n",
    "    #x_labels = ['0', '3', '4', '8', '9']\n",
    "    #y_labels = ['0', '3', '4', '8', '9']\n",
    "\n",
    "    #sb.heatmap(conf, annot = True, fmt='g',xticklabels = x_labels, yticklabels = y_labels, cbar=False)\n",
    "    \n",
    "    #fig = plt.figure(figsize=(12,12), num = 3)\n",
    "    \n",
    "    #plt.title('Actual Digit', fontsize = 10)\n",
    "    #plt.xlabel(f'{string} {hidden}', fontsize = 12)\n",
    "    #plt.ylabel('Predicted Digit', fontsize = 10)\n",
    "    #plt.tick_params(axis='both', which='major', labelsize=8, labelbottom = False, bottom=False, top = False, labeltop=True)\n",
    "    \n",
    "    #plt.savefig('./output/'+string+f'_{hidden}.png')\n",
    "    #plt.show()\n",
    "    #plt.clf()\n",
    "    #plt.close(fig)\n",
    "\n",
    "opt_dict = {0: 'SGD', 1: 'Batch Gradient Descent', 2: 'Momentum', 3: 'NAG', 4: 'Adagrad', 5: 'RMSProp', 6: 'Adam'}\n",
    "\n",
    "def report(model, hidden):\n",
    "  print('\\nValidation and Test Accuracy:')\n",
    "\n",
    "  print(model.evaluate(x = X_val, y = y_val, batch_size = None, verbose=0, callbacks = None)[1])\n",
    "  print(model.evaluate(x = X_test, y = y_test, batch_size = None, verbose=0, callbacks = None)[1], '\\n')\n",
    "\n",
    "  y_pred_train = model.predict(X_train, batch_size=None, verbose=0, callbacks=None)\n",
    "  y_pred_test = model.predict(X_test, batch_size=None, verbose=0, callbacks=None)\n",
    "\n",
    "  y_train_cl, y_pred_train_sgd_cl = conv_label(y_train, y_pred_train)\n",
    "  y_test_cl, y_pred_test_sgd_cl = conv_label(y_test, y_pred_test)\n",
    "\n",
    "  cm1 = tf.math.confusion_matrix(y_train_cl, y_pred_train_sgd_cl)\n",
    "  cm_plot(hidden, cm1, 'Training Confusion Matrix')\n",
    "  \n",
    "  cm2 = tf.math.confusion_matrix(y_test_cl, y_pred_test_sgd_cl)\n",
    "  cm_plot(hidden, cm2, 'Test Confusion Matrix')\n",
    "\n",
    "opt_dict = {0: 'SGD', 1: 'Batch Gradient Descent', 2: 'Momentum', 3: 'NAG', 4: 'Adagrad', 5: 'RMSProp', 6: 'Adam'}\n",
    "optimizers = [tf.keras.optimizers.SGD(learning_rate = 0.001, name='SGD'), \n",
    "              tf.keras.optimizers.SGD(learning_rate=0.001, name='BGD'),\n",
    "              tf.keras.optimizers.SGD(learning_rate = 0.001, momentum = 0.9, name = 'Momentum_SGD'),\n",
    "              tf.keras.optimizers.SGD(learning_rate = 0.001, momentum = 0.9, nesterov = True, name='NAG'),\n",
    "              tf.keras.optimizers.Adagrad(learning_rate = 0.001, epsilon = 1e-08, name = \"Adagrad\"),\n",
    "              tf.keras.optimizers.RMSprop(learning_rate = 0.001, rho = 0.99, momentum = 0.0, epsilon = 1e-08, name=\"RMSProp\"),\n",
    "              tf.keras.optimizers.Adam(learning_rate = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, name='Adam') ]\n",
    "\n",
    "my_callbacks = [EarlyStopping(monitor = 'loss', min_delta = 1e-04, patience = 1)]\n",
    "\n",
    "def train2(hidden):\n",
    "\n",
    "  save_w(hidden)\n",
    "\n",
    "  print('\\n',hidden, '\\n')\n",
    "  count = 0\n",
    "  hist_arr = []\n",
    "  acc_arr = []\n",
    "  epp = []\n",
    "  for optimizer in optimizers:\n",
    "\n",
    "      bs = 1\n",
    "      if count == 1:\n",
    "          bs = X_train.shape[0]\n",
    "      \n",
    "      model = architecture(hidden)\n",
    "      model.load_weights('./weights/'+f'model_{hidden}.h5')\n",
    "\n",
    "      # for layer in model.layers:\n",
    "      #     print(layer.get_weights()[0])\n",
    "      #     break\n",
    "\n",
    "      model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "      model_fit = model.fit(X_train, y_train, batch_size=bs, epochs=epochs, callbacks = my_callbacks, verbose=verbose)\n",
    "      loss, acc = model.evaluate(X_test, y_test, verbose = 0)\n",
    "      print(\"Optimizer: {}, Test Accuracy: {:.2f}%\".format(optimizer.__class__.__name__, acc * 100))\n",
    "\n",
    "      avg_err = model_fit.history['loss']\n",
    "\n",
    "      hist_arr.append(avg_err)\n",
    "      acc_arr.append(model_fit.history['accuracy'][len(avg_err) - 1])\n",
    "      epp.append(len(avg_err))\n",
    "      print(f'\\n-- REPORT: {opt_dict[count]}--')\n",
    "      report(model, hidden)\n",
    "\n",
    "      count = count + 1\n",
    "\n",
    "  print('\\nOrder:')\n",
    "  for i in opt_dict:\n",
    "      print(f'{opt_dict[i]}')\n",
    "\n",
    "  print('\\nTraining Accuracy: ')\n",
    "  for i in opt_dict:\n",
    "      print(f'{acc_arr[i]}')\n",
    "\n",
    "  print('\\nEpochs: ')\n",
    "  for i in opt_dict:\n",
    "      print(f'{epp[i]}')\n",
    "  \n",
    "\n",
    "  return hist_arr\n",
    "\n",
    "def plot_epoch(hidden, hist_arr):\n",
    "\n",
    "    #plt.figure(num = 1)\n",
    "   \n",
    "    #plt.plot(hist_arr[1])\n",
    "    #plt.title(\"Average Error vs Epoch\")\n",
    "    #plt.xlabel(\"Epoch\")\n",
    "    #plt.ylabel(\"Average Error\")\n",
    "    #plt.legend(['Batch Gradient Descent'], fontsize = 7, loc = 'upper right')\n",
    "    #plt.savefig('./output/'+f'bgd_{hidden}.png')\n",
    "    #plt.clf()\n",
    "    #plt.close()\n",
    "\n",
    "    #leg = ['SGD', 'Batch-GD', 'Momentum', 'NAG', 'Adagrad', 'RMSProp', 'Adam']\n",
    "\n",
    "    leg = ['SGD', 'Momentum', 'NAG', 'Adagrad', 'RMSProp', 'Adam']\n",
    "    \n",
    "    #plt.figure(num = 2)\n",
    "    #x = hist_arr.pop(1)\n",
    "    for i in range(len(hist_arr)):\n",
    "        plt.plot(hist_arr[i])\n",
    "   \n",
    "   \t\n",
    "    plt.title(\"Average Error vs Epoch \"+str(hidden))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Average Error\")\n",
    "    plt.legend(leg, fontsize = 7, loc = 'upper right')\n",
    "    plt.savefig('./output/'+f'epoch_{hidden}.png')\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def run(hidden):\n",
    "    hist_arr = train2(hidden)\n",
    "    plot_epoch(hidden, hist_arr)\n",
    "    \n",
    "run_list = [(4, 2, 1), (8, 4, 2), (32, 16, 8), (64, 32, 16), (512, 256, 64)]\n",
    "\n",
    "#run_list = [(8, 4, 2), (32, 16, 8), (64, 32, 16), (512, 256, 64), (1024, 512, 256), (2048, 1024, 512), (1000, 750, 250),  (100, 50, 25),\n",
    "#             (16, 8, 4, 2), (32, 16, 8, 4), (256, 64, 16, 4), (1024, 512, 256, 64), (2048, 1024, 512, 256), \n",
    "#             (64, 32, 16, 8, 4), (512, 64, 32, 16, 8), (2048, 1024, 512, 64, 8), (4096, 2048, 1024, 512, 256)]\n",
    "\n",
    "\n",
    "for i in run_list:\n",
    "    run(i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
